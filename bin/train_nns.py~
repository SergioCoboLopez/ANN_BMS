#25/4/2025 - A code to train neural networks without overfitting
import pyrenn
import numpy as np
import copy
import pandas as pd
import pickle
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import sys
import matplotlib.gridspec as gridspec
from sklearn.metrics import mean_squared_error
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import mean_absolute_error
from random import sample
import random

def build_validation(all_training_points,len_validation,dataframe):

    pre_train_df=dataframe.loc[0:all_training_points-1]
    
    validation_df=dataframe.iloc[validation_points]
    train_df=pre_train_df.drop(labels=validation_points)

    return train_df, validation_df


#Read data
#-----------------------------------------------------
random.seed(a=1111)

function=sys.argv[1] #'tanh',  'leaky_ReLU'
sigma=sys.argv[2]
realization=int(sys.argv[3])

resolution='0.5x' #1x, 2x, 0.5x, 4e-3x
resolutions={'0.5x':'0.1', '1x':'0.05', '2x': '0.025', '4e-3x':'0.004' }

output_path='../data/' + resolution + '_resolution/trained_nns/'
input_path= '../data/' + resolution + '_resolution/'
filename=input_path + 'NN_' + function + '_sigma_' + str(sigma) + '_r_' + str(realization) + \
    '_res_' + resolutions[resolution] + '.csv'


d=pd.read_csv(filename)
d=d.drop(columns='Unnamed: 0')

#Take subset of data
d=d[(d['x1'] >= -2.0) & (d['x1']<=2.0)]
d=d.reset_index(drop=True)

#train/validation size
#-----------------------------------------------------
n_points=int(len(d.index)/10)
pre_train_fraction=3/4;pre_train_size=int(n_points*pre_train_fraction)
validation_fraction=1/8;validation_size=int(n_points*validation_fraction)
validation_points=sample(range(pre_train_size), k=validation_size)
validation_points=np.sort(validation_points)

with open( output_path + 'validation_%s_n_%s_%s_r_%d' %(resolution, function, sigma, realization) + '.txt' \
     , 'a') as the_file:
    the_file.write(str(validation_points))
#-----------------------------------------------------

#Build ANN
ILS = 1;OLS=1
NL, LS = 5, 10
arch=[ILS] + NL*[LS] + [OLS]
nn=pyrenn.CreateNN(arch)

n_functions=int(d['rep'].max()) #Number of functions in dataset
iterations=300

for n in range(n_functions + 1):
    #Read data
    dn=d[d['rep']==n]
    dn.index.name = None
    dn=dn.reset_index(drop=True)

    train_set, validation_set=build_validation(pre_train_size, validation_points, dn)
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    xtrain=train_set['x1']
    ytrain=train_set['y_noise']

    xvalid=validation_set['x1']
    yvalid=validation_set['y_noise']
    
    #Error and neural network vectors
    MAE=[];MSE=[];RMSE=[]        #Lists of validation errors
    MAE_t=[];MSE_t=[]; RMSE_t=[] #List of training errors
    nn_dict={} #Dictionary of neural network models

#--------------------------------------------------
    for i in range(iterations):
        #Two iterations of training a neural network (k_max=1)
        net=pyrenn.train_LM(xtrain,ytrain,nn,verbose=True,k_max=1,E_stop=1e-200)
        
        #Test NN on validation set
        yvalid_test = pyrenn.NNOut(xtrain,net) #Prediction on train
        yvalid_pred = pyrenn.NNOut(xvalid,net) #Prediction on valid

        #Validation errors
        #--------------------------------------------------
        MSE_i=mean_squared_error(yvalid,yvalid_pred)
        MSE.append(MSE_i)

        MAE_i=mean_absolute_error(yvalid,yvalid_pred)
        MAE.append(MAE_i)

        RMSE_i=root_mean_squared_error(yvalid,yvalid_pred)
        RMSE.append(RMSE_i)
        #--------------------------------------------------

        #Training errors
        #--------------------------------------------------
        MSE_t_i=mean_squared_error(ytrain,yvalid_test)
        MSE_t.append(MSE_t_i)

        MAE_t_i=mean_absolute_error(ytrain,yvalid_test)
        MAE_t.append(MAE_t_i)
        
        RMSE_t_i=root_mean_squared_error(ytrain,yvalid_test)
        RMSE_t.append(RMSE_t_i)
        #--------------------------------------------------

        #deepcopy and save neural network to dictionary
        net_copy=copy.deepcopy(net)
        nn_dict[i]=net_copy
        
        #update neural network for next step of the loop
        nn=net
#--------------------------------------------------
        
    #Find the model with the minimum error
    min_error_mse=min(MSE);min_error_rmse=min(RMSE)

    #Take indices of the elements with minimum error
    min_err_mse_ind=MSE.index(min_error_mse);min_err_rmse_ind=RMSE.index(min_error_rmse)
    #--------------------------------------------------------

    #Plot train and validation errors
    #----------------------------------------------------

    #Figure settings
    #--------------------------------
    output_path_fig='../results/nn_w_validation/'
    name_fig='validation_errors_' + 'sigma_' + str(sigma) + '_' + str(function) + '_' + str(n) + '_r_' + str(realization) + '.png'
    
    #Define figure size
    cm = 1/2.54 #convert inch to cm                                  
    width = 12*cm; height=10*cm
    fig=figure(figsize=(width,height), dpi=300)

    #Fonts and sizes
    size_axis=7;size_ticks=6;size_title=5
    line_w=1;marker_s=3
    #--------------------------------
    plt.plot(MAE, '.', markersize=6, color='blue', label='MAE validation')
    plt.plot(RMSE,'.',markersize=6,color='green',label='RMSE validation')

    plt.plot(MAE_t, linewidth=1,linestyle='--',color='blue',label='MAE train')
    plt.plot(RMSE_t,linewidth=1,linestyle='--',color='green',label='RMSE train')
    plt.scatter(min_err_rmse_ind,min_error_rmse,s=80,marker='*',color='red',label='minimum rmse')
    #--------------------------------------------------------
    #Labels
    plt.legend(loc='best', fontsize=size_ticks)
    plt.xlabel('iterations',fontsize=size_axis);plt.ylabel('error',fontsize=size_axis)
    plt.title('%s, n=%d' % (function, n),fontsize=size_title)
    plt.savefig(output_path_fig+name_fig,dpi=300)
    #----------------------------------------------------
    
    #Best nn found
    #------------------------------------------------------
    net_best=nn_dict[min_err_rmse_ind]
    xtest = dn.loc[pre_train_size:]['x1']
    
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                            
    xtrain_valid=dn.loc[:pre_train_size-1]['x1']

    #ytest_best = pyrenn.NNOut(xtrain,net_best) #old version                                        
    ytest_best = pyrenn.NNOut(xtrain_valid,net_best) #new version                                   
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    ypred_best = pyrenn.NNOut(xtest,net_best)
    ymodel_best=np.concatenate((ytest_best, ypred_best))
    #------------------------------------------------------

    #Save neural network
    pyrenn.saveNN(net_best, output_path + 'NN_weights_no_overfit_' + function + '_sigma_' + str(sigma) + '_rep_' + str(n) + '_r_' + str(realization) + '.csv')


    try:
        ymodel=np.append(ymodel,ymodel_best)
    except NameError:
        ymodel=ymodel_best


#Add predictions to data
d['ymodel']=ymodel

#Save updated data with model
d.to_csv( output_path + 'NN_no_overfit_' + function + '_sigma_' + str(sigma) + '_r_' + str(realization) + '.csv')
